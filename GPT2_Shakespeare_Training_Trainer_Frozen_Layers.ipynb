{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQeNYeYvEh1c",
        "outputId": "489f56db-4e6c-4673-cee9-0eb08b77ba86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UhUCKWEYE2gE"
      },
      "outputs": [],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fl_S9eTGFpeL"
      },
      "outputs": [],
      "source": [
        "data_file_open = open('/content/data_f.txt', 'r', encoding='UTF-8')\n",
        "label_file_open = open('/content/label_f.txt', 'r', encoding='UTF-8')\n",
        "data_lines=data_file_open.readlines()\n",
        "label_lines = label_file_open.readlines()\n",
        "\n",
        "input_dataset = []\n",
        "output_dataset = []\n",
        "dataset = []\n",
        "\n",
        "\n",
        "for item in data_lines:\n",
        "    input_dataset.append(item.strip())\n",
        "for item in label_lines:\n",
        "    output_dataset.append(item.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJoLXs--dRlz",
        "outputId": "f35bb573-7d3c-4221-d1c6-e47d61cec9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42535\n",
            "42535\n"
          ]
        }
      ],
      "source": [
        "print(len(input_dataset))\n",
        "print(len(output_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SO69IDwbF5hA"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for i in range(len(input_dataset)):\n",
        "    data = input_dataset[i]+' = '+output_dataset[i]\n",
        "    #if(len(data)>200):\n",
        "    #    continue\n",
        "    #if(len(data)<20):\n",
        "    #    continue\n",
        "    dataset.append(data)"
      ],
      "metadata": {
        "id": "vfpDVmFY8nuj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2)"
      ],
      "metadata": {
        "id": "yZV0l6dxHqHu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX98fI6MH1qz",
        "outputId": "9f432056-d782-4a07-991f-57c76c593475"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34028\n",
            "8507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_train = []\n",
        "\n",
        "tokenized_dataset_val = []\n",
        "\n",
        "\n",
        "for data in train_dataset:\n",
        "    tokenized_dataset_train.append(tokenizer.encode(data))\n",
        "\n",
        "for data in val_dataset:\n",
        "    tokenized_dataset_val.append(tokenizer.encode(data))"
      ],
      "metadata": {
        "id": "RXY6y4yvAwsg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "cMLFjONt_tD9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4KDJbI9Lafhd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.named_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKhMTDGLarOW",
        "outputId": "f3e290d3-e7ea-4f87-b8c5-a47a4254f16b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.named_parameters at 0x7f3adb92f3d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"transformer.h.9\"):\n",
        "        param.requires_grad = True\n",
        "    elif name.startswith(\"transformer.h.10\"):\n",
        "        param.requires_grad = True\n",
        "    elif name.startswith(\"transformer.h.11\"):\n",
        "        param.requires_grad = True\n",
        "    elif name.startswith(\"transformer.ln_f\"):\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "3Uo6MmP5afem"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"transformer.ln_f\"):\n",
        "        print(name)\n",
        "        print(param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7CPr5FcbxzI",
        "outputId": "92018072-f2bf-4b4f-ea76-63c4c0cceebc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.ln_f.weight\n",
            "True\n",
            "transformer.ln_f.bias\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "68bwkyuGbiPj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cN-4dG13afbw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "          output_dir='/content/train_2',\n",
        "          learning_rate=8e-5,\n",
        "          per_device_train_batch_size=8,\n",
        "          per_device_eval_batch_size=8,\n",
        "          num_train_epochs=5,\n",
        "          evaluation_strategy = \"epoch\",\n",
        "          save_strategy='epoch'\n",
        "      )"
      ],
      "metadata": {
        "id": "zZs4gSJVBkHH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          train_dataset=tokenized_dataset_train,\n",
        "          eval_dataset=tokenized_dataset_val,\n",
        "          data_collator=data_collator)"
      ],
      "metadata": {
        "id": "oAdciSSsB2oG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fKrLdMTqCe6W",
        "outputId": "ab3e21b3-7a85-4f1e-a6c3-075d88160de7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 34028\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 21270\n",
            "  Number of trainable parameters = 21265152\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21270' max='21270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21270/21270 30:42, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.282200</td>\n",
              "      <td>3.078888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.165300</td>\n",
              "      <td>3.010662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.065000</td>\n",
              "      <td>2.985982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.034800</td>\n",
              "      <td>2.972035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.963100</td>\n",
              "      <td>2.969222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 8507\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/train_2/checkpoint-4254\n",
            "Configuration saved in /content/train_2/checkpoint-4254/config.json\n",
            "Model weights saved in /content/train_2/checkpoint-4254/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8507\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/train_2/checkpoint-8508\n",
            "Configuration saved in /content/train_2/checkpoint-8508/config.json\n",
            "Model weights saved in /content/train_2/checkpoint-8508/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8507\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/train_2/checkpoint-12762\n",
            "Configuration saved in /content/train_2/checkpoint-12762/config.json\n",
            "Model weights saved in /content/train_2/checkpoint-12762/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8507\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/train_2/checkpoint-17016\n",
            "Configuration saved in /content/train_2/checkpoint-17016/config.json\n",
            "Model weights saved in /content/train_2/checkpoint-17016/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8507\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/train_2/checkpoint-21270\n",
            "Configuration saved in /content/train_2/checkpoint-21270/config.json\n",
            "Model weights saved in /content/train_2/checkpoint-21270/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=21270, training_loss=3.1426242327992773, metrics={'train_runtime': 1844.4508, 'train_samples_per_second': 92.244, 'train_steps_per_second': 11.532, 'total_flos': 4490748582912000.0, 'train_loss': 3.1426242327992773, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xrKQmfEChmm",
        "outputId": "31e5b453-3c24-43e7-a37d-c3f55e738d3e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/train_2\n",
            "Configuration saved in /content/train_2/config.json\n",
            "Model weights saved in /content/train_2/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('/content/train_2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHfaNMA8nse",
        "outputId": "41a8a604-1648-4bb1-a19e-d0d565621461"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/train_2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file /content/train_2/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/train_2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Zw6NhKRJUOKy",
        "outputId": "4a59d63d-51ed-40d6-b2a6-c3c83a378160"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, bloody cloth, I'll keep you, because I wanted you to be this color. = Yea, bloody cloth, I'll keep thee, for I wish'd Thou shouldst be colour'd thus.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset[99]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lBCPzN7cT2Be",
        "outputId": "de29a12a-8eaa-4e32-f7b4-489cbe0210bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Freedom! = Freedom!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode(input_dataset[0]+' = ')"
      ],
      "metadata": {
        "id": "BTSKyJ2C8nqH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode('Hello, how are you? = ')\n",
        "\n",
        "output = model.generate(input_ids=torch.tensor([input]),max_new_tokens=30, do_sample=True)\n",
        "\n",
        "tokenizer.decode(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Q_DyfOkkUifT",
        "outputId": "99c54f71-5d4c-4654-f9e2-387461085649"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, how are you? =  How now, how fares you?  and so fare thee?  Is this how? sayest thou? what are thy powers?  Is this'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode('You were either ignorant of it or, seeing it, why were you so childishly friendly? = ')\n",
        "\n",
        "output = model.generate(input_ids=torch.tensor([input]),max_new_tokens=30, do_sample=True)\n",
        "\n",
        "tokenizer.decode(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0uhOZy1uVc6n",
        "outputId": "c169c1b5-11ba-4358-f45c-38a2680eed78"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You were either ignorant of it or, seeing it, why were you so childishly friendly? =  Or were you, either unaware, or, seeing it, Why be so unkind to the young?  Or, knowing it, how do you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode('I will always follow your instructions. = ')\n",
        "\n",
        "output = model.generate(input_ids=torch.tensor([input]),max_new_tokens=30, do_sample=True)\n",
        "\n",
        "tokenizer.decode(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "T7IZl1g5VoYN",
        "outputId": "c2d79654-f351-4c89-e8e5-c963299366b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I will always follow your instructions. = .  \" = I will always follow thy instructions.  \\'Faithful!  \\'By thy office, my lord.  \\'By thy will'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode('We shall leave tomorrow. = ')\n",
        "\n",
        "output = model.generate(input_ids=torch.tensor([input]),max_new_tokens=30, do_sample=True)\n",
        "\n",
        "tokenizer.decode(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wHCvvWDdWFpo",
        "outputId": "45c3755a-8bee-4722-dc9f-9d7d3fb38512"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We shall leave tomorrow. =  Tomorrow shall we depart.  Next day shall we yet be gone. = Tomorrow tomorrow shall we now be departed.   Tomorrow shall we tomorrow be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode('Let\\'s see, will his finger catch fire? = ')\n",
        "\n",
        "output = model.generate(input_ids=torch.tensor([input]),max_new_tokens=30, do_sample=True)\n",
        "\n",
        "tokenizer.decode(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5Tx75Ev6gevC",
        "outputId": "2943d3b6-3fef-4c01-8023-afac09026a79"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's see, will his finger catch fire? =  'Twill show him fire, to the finger of his master?  'Tis his finger.  'Let us observe, though his finger\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode('Yes, bloody cloth, I\\'ll keep you, because I wanted you to be this color. = ')\n",
        "\n",
        "output = model.generate(input_ids=torch.tensor([input]),max_new_tokens=30, do_sample=True)\n",
        "\n",
        "tokenizer.decode(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ksvV49J3u-BA",
        "outputId": "3dd3d7b5-c3a0-476a-be3f-cdadd8577a12"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, bloody cloth, I'll keep you, because I wanted you to be this color. =  'Do, bloody cloth, For I had intended thee to be this color, for there I would be.  'Thou must be damned with\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IJrZd4xXgYF",
        "outputId": "b61b6183-9c1b-4295-cdc7-33e032fc3287"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"/content/train_2/config.json\",\"/content/drive/MyDrive/ECE1786_Project/Models/GPT2_4Layers_8e-5/\")\n",
        "shutil.copy(\"/content/train_2/pytorch_model.bin\",\"/content/drive/MyDrive/ECE1786_Project/Models/GPT2_4Layers_8e-5/\")\n",
        "shutil.copy(\"/content/train_2/training_args.bin\",\"/content/drive/MyDrive/ECE1786_Project/Models/GPT2_4Layers_8e-5/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cUBoCorrYP2_",
        "outputId": "c2134263-88ce-427d-c702-05ba3dfd4222"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/ECE1786_Project/Models/GPT2_4Layers_8e-5/training_args.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qpf6ihc6YqTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IOYWPAvZ0Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcJ-pLz8Z3pK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}